{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # 추가: OpenCV 라이브러리\n",
    "\n",
    "# Grad-CAM 구현을 위한 클래스 정의\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradient = None\n",
    "        self.activation = None\n",
    "        \n",
    "        # 후킹 함수\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activation = output\n",
    "            \n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradient = grad_out[0]\n",
    "        \n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_backward_hook(backward_hook)\n",
    "    \n",
    "    def __call__(self, x, class_idx=None):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(x)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax().item()\n",
    "        \n",
    "        target = output[0][class_idx]\n",
    "        target.backward()\n",
    "        \n",
    "        gradient = self.gradient[0].cpu().data.numpy()\n",
    "        activation = self.activation[0].cpu().data.numpy()\n",
    "        \n",
    "        weights = np.mean(gradient, axis=(1, 2))\n",
    "        cam = np.zeros(activation.shape[1:], dtype=np.float32)\n",
    "        \n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activation[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam / cam.max()\n",
    "        return cam\n",
    "\n",
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        # train_aug에 있는 파일 이름 형식으로 변환\n",
    "        matching_name = [f for f in os.listdir(self.path) if f.endswith(name)]\n",
    "        if not matching_name:\n",
    "            raise FileNotFoundError(f\"File not found: {name}\")\n",
    "        img_path = os.path.join(self.path, matching_name[0])\n",
    "        img = np.array(Image.open(img_path))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate(loader, model, loss_fn, device, target_layer, train_df):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    incorrect_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            for target, pred, img_name in zip(targets_np, preds_np, loader.dataset.df[:, 0]):\n",
    "                if target != pred:\n",
    "                    img_path = os.path.join(loader.dataset.path, img_name)\n",
    "                    actual_target = int(train_df[train_df['filename'] == img_name]['target'].values[0])\n",
    "                    incorrect_samples.append((actual_target, int(pred), img_path))\n",
    "\n",
    "    # 잘못된 예측에 대해 Grad-CAM 시각화\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    os.makedirs(\"/root/incorrect_images_CAM/\", exist_ok=True)\n",
    "    for target, pred, img_path in incorrect_samples:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        src_path = os.path.join(\"/root/data/train/\", img_name)  # train 디렉토리에서 이미지 경로\n",
    "        img = Image.open(src_path).convert('RGB')\n",
    "        img = img.resize((img_size, img_size))\n",
    "        img_tensor = transform(image=np.array(img))['image'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Grad-CAM 시각화\n",
    "        cam = grad_cam(img_tensor, class_idx=pred)\n",
    "\n",
    "        # 원본 이미지와 Grad-CAM 결과 시각화\n",
    "        image_np = img_tensor.cpu().data.numpy()[0].transpose(1, 2, 0)\n",
    "        image_np = np.array([0.229, 0.224, 0.225]) * image_np + np.array([0.485, 0.456, 0.406])\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "        # CAM을 원본 이미지 크기에 맞게 변환\n",
    "        cam = np.uint8(255 * cam)\n",
    "        cam = np.uint8(Image.fromarray(cam).resize((image_np.shape[1], image_np.shape[0]), Image.LANCZOS))\n",
    "\n",
    "        # OpenCV로 색상 맵 적용\n",
    "        heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "\n",
    "        # BGR을 RGB로 변환\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 원본 이미지 위에 히트맵 겹치기\n",
    "        superimposed_img = heatmap * 0.4 + np.uint8(image_np * 255)\n",
    "\n",
    "        # 이미지를 0-1 범위로 정규화\n",
    "        superimposed_img = np.clip(superimposed_img / 255.0, 0, 1)\n",
    "\n",
    "        # 시각화 이미지 저장\n",
    "        cam_path = os.path.join(\"/root/incorrect_images_CAM/\", f\"cam_{target}_{pred}_{img_name}\")\n",
    "        plt.imsave(cam_path, superimposed_img)\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"incorrect_samples\": incorrect_samples\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# Hyper-parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '/root/data/'\n",
    "model_name = 'efficientnet_b0'\n",
    "img_size = 224\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "n_splits = 5\n",
    "patience = 3\n",
    "\n",
    "# Transform 정의\n",
    "transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"/root/data/train.csv\")\n",
    "\n",
    "best_overall_model = None\n",
    "best_overall_f1 = 0\n",
    "best_overall_loss = float('inf')\n",
    "best_fold_idx = -1\n",
    "\n",
    "all_incorrect_samples = []  # 모든 폴드의 잘못된 예측을 저장할 리스트\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_weights = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, \"/root/data/train_aug/\", transform=transform)\n",
    "    val_dataset = ImageDataset(val_df, \"/root/data/train_aug/\", transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=len(df['target'].unique())\n",
    "    ).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_f1 = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Grad-CAM 시각화를 위한 모델 준비\n",
    "    target_layer = model.conv_head\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device=device)\n",
    "        val_ret = evaluate(val_loader, model, loss_fn, device=device, target_layer=target_layer, train_df=df)\n",
    "        val_ret['epoch'] = epoch\n",
    "\n",
    "        log = f\"Epoch {epoch + 1}/{EPOCHS}\\n\"\n",
    "        for k, v in train_ret.items():\n",
    "            log += f\"Train {k}: {v:.4f}\\n\"\n",
    "        for k, v in val_ret.items():\n",
    "            if k != 'incorrect_samples':  # 잘못된 예측 샘플은 로그에서 제외\n",
    "                log += f\"Val {k}: {v:.4f}\\n\"\n",
    "        print(log)\n",
    "\n",
    "        if val_ret['val_loss'] < best_val_loss or val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_loss = val_ret['val_loss']\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            torch.save(model.state_dict(), f\"best_model_fold_{fold + 1}_{model_name}.pth\")\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # 현재 폴드의 최상의 F1 점수를 기록\n",
    "    fold_weights.append(val_ret['val_f1'])\n",
    "    # 현재 폴드의 잘못된 예측을 저장\n",
    "    all_incorrect_samples.extend(val_ret['incorrect_samples'])\n",
    "\n",
    "    # 모델별 최상의 성능 비교 및 업데이트\n",
    "    if best_val_f1 > best_overall_f1 and best_val_loss < best_overall_loss:\n",
    "        best_overall_f1 = best_val_f1\n",
    "        best_overall_loss = best_val_loss\n",
    "        best_fold_idx = fold + 1\n",
    "\n",
    "# 잘못된 예측 결과를 저장\n",
    "incorrect_df = pd.DataFrame(all_incorrect_samples, columns=['target', 'pred', 'img_path'])\n",
    "incorrect_df.to_csv(\"/root/incorrect_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"Best Model from Fold {best_fold_idx} with F1: {best_overall_f1} and Loss: {best_overall_loss}\")\n",
    "\n",
    "# 예측 및 결과 저장\n",
    "test_df = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
    "tst_dataset = ImageDataset(test_df, \"/root/data/test/\", transform=transform)\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=len(df['target'].unique())\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"best_model_fold_{best_fold_idx}_{model_name}.pth\"))\n",
    "model.eval()\n",
    "\n",
    "fold_preds = []\n",
    "for image, _ in tqdm(tst_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    fold_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "fold_preds = np.concatenate(fold_preds, axis=0)\n",
    "final_preds = np.argmax(fold_preds, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame(test_df, columns=['ID'])\n",
    "pred_df['target'] = final_preds\n",
    "sample_submission_df = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()\n",
    "pred_df.to_csv(\"/root/efficient_net_test.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
