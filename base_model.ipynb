{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# PATH\n",
    "TRAIN_AUG_CSV_PATH = '/upstage-cv-classification-cv2/data/train_aug.csv'\n",
    "TRAIN_AUG_IMAGE_PATH = '/upstage-cv-classification-cv2/data/train_aug'\n",
    "\n",
    "VALID_CSV_PATH = '/upstage-cv-classification-cv2/data/valid.csv'\n",
    "VALID_IMAGE_PATH = '/upstage-cv-classification-cv2/data/valid'\n",
    "\n",
    "TEST_CSV_PATH = '/upstage-cv-classification-cv2/data/sample_submission.csv'\n",
    "TEST_IMAGE_PATH = '/upstage-cv-classification-cv2/data/test'\n",
    "\n",
    "RESULT_CSV_PATH = '/upstage-cv-classification-cv2'\n",
    "\n",
    "WANDB_PROJECT_NAME = 'cv_competition_base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "img_size = 380\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "\n",
    "patience = 5\n",
    "min_delta = 0.001 # 성능 개선의 최소 변화량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26690 3140\n"
     ]
    }
   ],
   "source": [
    "# test image 변환\n",
    "data_transform = A.Compose([\n",
    "    A.Resize(height = img_size, width = img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image = img)['image']\n",
    "    \n",
    "        return img, target\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.df[:, 1] \n",
    "\n",
    "trn_dataset = ImageDataset(\n",
    "    TRAIN_AUG_CSV_PATH,\n",
    "    TRAIN_AUG_IMAGE_PATH,\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    VALID_CSV_PATH,\n",
    "    VALID_IMAGE_PATH,\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "tst_dataset = ImageDataset(\n",
    "    TEST_CSV_PATH,\n",
    "    TEST_IMAGE_PATH,\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "labels = trn_dataset.get_labels()\n",
    "labels = labels.astype(int)\n",
    "\n",
    "# DataLoader\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = 0,\n",
    "    pin_memory = True,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 0,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "print(len(trn_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b4.ra2_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b4.ra2_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = timm.create_model('efficientnet_b4',\n",
    "                        pretrained=True,\n",
    "                        num_classes = 17).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr = LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch 학습\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list =[]\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for step, (image, targets) in enumerate(pbar):\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none = True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_step\" : epoch * len(loader) + step,\n",
    "            \"train_loss_step\" : loss.item()\n",
    "        })\n",
    "        \n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average = 'macro')\n",
    "\n",
    "    ret = {\n",
    "        \"model\" : model,\n",
    "        \"train_epoch\" : epoch,\n",
    "        \"train_loss\" : train_loss,\n",
    "        \"tarin_acc\" : train_acc,\n",
    "        \"train_f1\" : train_f1\n",
    "    }\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_epoch\" : epoch,\n",
    "        \"train_loss_epoch\" : train_loss,\n",
    "        \"train_acc\" : train_acc,\n",
    "        \"train_f1\" : train_f1\n",
    "    })\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(loader, model, loss_fn, device, epoch):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "\n",
    "    preds_list =[]\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for step, (image, targets) in enumerate(pbar):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "       \n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "            wandb.log({\n",
    "                \"valid_step\" : epoch * len(loader) + step,\n",
    "                \"valid_loss_step\" : loss.item()\n",
    "            })\n",
    "\n",
    "    valid_loss /= len(loader)\n",
    "    valid_acc = accuracy_score(targets_list, preds_list)\n",
    "    valid_f1 = f1_score(targets_list, preds_list, average = 'macro')\n",
    "\n",
    "    ret = {\n",
    "        \"epoch\" : epoch,\n",
    "        \"valid_loss\" : valid_loss,\n",
    "        \"valid_acc\" : valid_acc,\n",
    "        \"valid_f1\" : valid_f1\n",
    "    }\n",
    "\n",
    "    wandb.log({\n",
    "        \"valid_epoch\" : epoch,\n",
    "        \"val_loss_epoch\" : valid_loss,\n",
    "        \"val_acc\" : valid_acc,\n",
    "        \"val_f1\" : valid_f1\n",
    "    })\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc240a470c446b5925099db09867577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112665178047286, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1951: 100%|██████████| 835/835 [02:25<00:00,  5.75it/s]\n",
      "Loss: 0.8145: 100%|██████████| 10/10 [00:00<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 0.9209675282239914\n",
      "valid f1 : 0.7973231943020411\n",
      "1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.9228: 100%|██████████| 835/835 [02:25<00:00,  5.73it/s]\n",
      "Loss: 0.0254: 100%|██████████| 10/10 [00:00<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 0.5328848786652088\n",
      "valid f1 : 0.8515226187326626\n",
      "2 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1151: 100%|██████████| 835/835 [02:25<00:00,  5.74it/s]\n",
      "Loss: 0.4211: 100%|██████████| 10/10 [00:00<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 0.574249729514122\n",
      "valid f1 : 0.8638497228897944\n",
      "3 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0336: 100%|██████████| 835/835 [02:26<00:00,  5.69it/s]\n",
      "Loss: 1.4203: 100%|██████████| 10/10 [00:00<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 5.000928801298142\n",
      "valid f1 : 0.8413033567983912\n",
      "4 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0041: 100%|██████████| 835/835 [02:25<00:00,  5.74it/s]\n",
      "Loss: 0.4983: 100%|██████████| 10/10 [00:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 0.7800617843866349\n",
      "valid f1 : 0.8546008669597089\n",
      "5 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0206: 100%|██████████| 835/835 [02:26<00:00,  5.69it/s]\n",
      "Loss: 1.8408: 100%|██████████| 10/10 [00:00<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 1.0815544307231904\n",
      "valid f1 : 0.8488437379635189\n",
      "6 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.4601: 100%|██████████| 835/835 [02:28<00:00,  5.62it/s]\n",
      "Loss: 1.4670: 100%|██████████| 10/10 [00:00<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss : 1.1022712230682372\n",
      "valid f1 : 0.8122842810393381\n",
      "Early stopping at epoch 6\n"
     ]
    }
   ],
   "source": [
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "\n",
    "f1_scores = []\n",
    "valid_losses = []\n",
    "trained_models = []\n",
    "patient_counter = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "wandb.init(project=WANDB_PROJECT_NAME, name=\"efficientenet_b4_base\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"{epoch} epoch\")\n",
    "    trn_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch)\n",
    "    val_ret =  valid_one_epoch(val_loader, model, loss_fn, device, epoch)\n",
    "\n",
    "    f1_scores.append(val_ret['valid_f1'])\n",
    "    valid_losses.append(val_ret['valid_loss'])\n",
    "    trained_models.append(trn_ret['model'])\n",
    "\n",
    "    print(f\"valid loss : {val_ret['valid_loss']}\")\n",
    "    print(f\"valid f1 : {val_ret['valid_f1']}\")\n",
    "\n",
    "    # 성능 개선 됨\n",
    "    if val_ret['valid_loss'] < best_valid_loss - min_delta:\n",
    "        best_valid_loss = val_ret['valid_loss']\n",
    "        patience_counter = 0  \n",
    "        \n",
    "    # 성능 개선 되지 않음\n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "\n",
    "    # 성능 개선이 patience 만큼 안되면 학습 중단\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "best_model_idx = np.argmin(np.array(valid_losses))\n",
    "best_model = trained_models[best_model_idx]\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.10it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "for image, _ in tqdm(tst_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = best_model(image)\n",
    "        \n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(f\"{RESULT_CSV_PATH}/base.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(valid_preds_list)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Confusion Matrix 생성\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 클래스의 최대값 확인 (히트맵의 크기를 결정하기 위해)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cm\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/envs/myvenv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/myvenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    248\u001b[0m     {\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m/opt/conda/envs/myvenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    114\u001b[0m             type_true, type_pred\n\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and multiclass targets"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 예측 결과 생성\n",
    "valid_preds_list = []\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "for image, _ in tqdm(val_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = best_model(image)\n",
    "        \n",
    "    valid_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 실제 레이블과 예측 레이블 준비\n",
    "true_labels = val_dataset.get_labels()\n",
    "pred_labels = np.array(valid_preds_list)\n",
    "\n",
    "# Confusion Matrix 생성\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# 클래스의 최대값 확인 (히트맵의 크기를 결정하기 위해)\n",
    "n_classes = max(cm.shape)\n",
    "\n",
    "# 히트맵 생성\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(n_classes), \n",
    "            yticklabels=range(n_classes))\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# x축과 y축의 눈금을 1단위로 설정\n",
    "plt.xticks(np.arange(0, n_classes, 1))\n",
    "plt.yticks(np.arange(0, n_classes, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
