{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     82\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 83\u001b[0m best_image \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_orientation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m best_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest oriented image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mfind_best_orientation\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_best_orientation\u001b[39m(image_path):\n\u001b[1;32m     51\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     preprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     angles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m270\u001b[39m]\n\u001b[1;32m     55\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     35\u001b[0m denoised \u001b[38;5;241m=\u001b[39m denoise_image(image)\n\u001b[1;32m     36\u001b[0m deblurred \u001b[38;5;241m=\u001b[39m deblur_image(denoised)\n\u001b[0;32m---> 37\u001b[0m contrast_enhanced \u001b[38;5;241m=\u001b[39m \u001b[43menhance_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeblurred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m restored_image \u001b[38;5;241m=\u001b[39m remove_stains(contrast_enhanced)\n\u001b[1;32m     39\u001b[0m restored_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(restored_image)\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36menhance_contrast\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance_contrast\u001b[39m(image):\n\u001b[0;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     enhanced_image \u001b[38;5;241m=\u001b[39m exposure\u001b[38;5;241m.\u001b[39mequalize_adapthist(image, clip_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (enhanced_image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (5,5) into shape (5,5,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     90\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 91\u001b[0m best_image \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_orientation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m best_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest oriented image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mfind_best_orientation\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_best_orientation\u001b[39m(image_path):\n\u001b[1;32m     59\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     preprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     angles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m270\u001b[39m]\n\u001b[1;32m     63\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     32\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m     34\u001b[0m denoised \u001b[38;5;241m=\u001b[39m denoise_image(image)\n\u001b[0;32m---> 35\u001b[0m deblurred \u001b[38;5;241m=\u001b[39m \u001b[43mdeblur_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoised\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(deblurred\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     38\u001b[0m     deblurred \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(deblurred, cv2\u001b[38;5;241m.\u001b[39mCOLOR_GRAY2BGR)\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mdeblur_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeblur_image\u001b[39m(image):\n\u001b[1;32m     18\u001b[0m     psf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m---> 19\u001b[0m     deconvolved, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrestoration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsupervised_wiener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (deconvolved \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/skimage/restoration/deconvolution.py:262\u001b[0m, in \u001b[0;36munsupervised_wiener\u001b[0;34m(image, psf, reg, user_params, is_real, clip, rng)\u001b[0m\n\u001b[1;32m    259\u001b[0m reg \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39mastype(float_type, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m psf\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m reg\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 262\u001b[0m     trans_fct \u001b[38;5;241m=\u001b[39m \u001b[43muft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mir2tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     trans_fct \u001b[38;5;241m=\u001b[39m psf\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/skimage/restoration/uft.py:396\u001b[0m, in \u001b[0;36mir2tf\u001b[0;34m(imp_resp, shape, dim, is_real)\u001b[0m\n\u001b[1;32m    394\u001b[0m irpadded_dtype \u001b[38;5;241m=\u001b[39m _supported_float_type(imp_resp\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    395\u001b[0m irpadded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape, dtype\u001b[38;5;241m=\u001b[39mirpadded_dtype)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mirpadded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimp_resp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imp_resp\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Roll for zero convention of the fft to avoid the phase\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# problem. Work with odd and even size.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, axis_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(imp_resp\u001b[38;5;241m.\u001b[39mshape):\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (5,5) into shape (5,5,3)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle: 0, Readability Score: 0\n",
      "Angle: 90, Readability Score: 0\n",
      "Angle: 180, Readability Score: 0\n",
      "Angle: 270, Readability Score: 0\n",
      "Flipped Angle: 0, Readability Score: 0\n",
      "Flipped Angle: 90, Readability Score: 0\n",
      "Flipped Angle: 180, Readability Score: 0\n",
      "Flipped Angle: 270, Readability Score: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m best_image \u001b[38;5;241m=\u001b[39m find_best_orientation(image_path)\n\u001b[0;32m---> 93\u001b[0m \u001b[43mbest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest oriented image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/best_oriented_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage import exposure, restoration\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Load the pre-trained Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def denoise_image(image):\n",
    "    return cv2.fastNlMeansDenoisingColored(image, None, 30, 30, 7, 21)\n",
    "\n",
    "def deblur_image(image):\n",
    "    psf = np.ones((5, 5)) / 25\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    deconvolved, _ = restoration.unsupervised_wiener(image, psf)\n",
    "    return (deconvolved * 255).astype(np.uint8)\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    enhanced_image = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    return (enhanced_image * 255).astype(np.uint8)\n",
    "\n",
    "def remove_stains(image):\n",
    "    mask = cv2.inRange(image, np.array([0, 0, 0]), np.array([60, 60, 60]))\n",
    "    inpainted_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n",
    "    return inpainted_image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    \n",
    "    denoised = denoise_image(image)\n",
    "    deblurred = deblur_image(denoised)\n",
    "    \n",
    "    contrast_enhanced = enhance_contrast(deblurred)\n",
    "    if len(contrast_enhanced.shape) == 2:\n",
    "        contrast_enhanced = cv2.cvtColor(contrast_enhanced, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    restored_image = remove_stains(contrast_enhanced)\n",
    "    if len(restored_image.shape) == 2:\n",
    "        restored_image = cv2.cvtColor(restored_image, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    restored_image = Image.fromarray(restored_image)\n",
    "    restored_image = ImageEnhance.Contrast(restored_image).enhance(2)\n",
    "    restored_image = ImageEnhance.Brightness(restored_image).enhance(1.5)\n",
    "    \n",
    "    return restored_image\n",
    "\n",
    "def recognize_text(image):\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "def find_best_orientation(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = preprocess_image(img)\n",
    "\n",
    "    angles = [0, 90, 180, 270]\n",
    "    best_score = 0\n",
    "    best_transformation = None\n",
    "\n",
    "    for angle in angles:\n",
    "        rotated_image = preprocessed_image.rotate(angle, expand=True)\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Angle: {angle}, Readability Score: {readability_score}\")\n",
    "\n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "\n",
    "    flipped_image = preprocessed_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    for angle in angles:\n",
    "        rotated_image = flipped_image.rotate(angle, expand=True)\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Flipped Angle: {angle}, Readability Score: {readability_score}\")\n",
    "\n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "\n",
    "    return best_transformation\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "best_image = find_best_orientation(image_path)\n",
    "best_image.save('dj/best_oriented_image.jpg')\n",
    "print(\"Best oriented image saved to 'dj/best_oriented_image.jpg'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle: 0, Readability Score: 0\n",
      "Angle: 90, Readability Score: 0\n",
      "Angle: 180, Readability Score: 0\n",
      "Angle: 270, Readability Score: 0\n",
      "Flipped Angle: 0, Readability Score: 0\n",
      "Flipped Angle: 90, Readability Score: 0\n",
      "Flipped Angle: 180, Readability Score: 0\n",
      "Flipped Angle: 270, Readability Score: 0\n",
      "Best oriented image saved to 'dj/best_oriented_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage import exposure, restoration\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Load the pre-trained Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def denoise_image(image):\n",
    "    return cv2.fastNlMeansDenoisingColored(image, None, 30, 30, 7, 21)\n",
    "\n",
    "def deblur_image(image):\n",
    "    psf = np.ones((5, 5)) / 25\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    deconvolved, _ = restoration.unsupervised_wiener(image, psf)\n",
    "    return (deconvolved * 255).astype(np.uint8)\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    enhanced_image = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
    "    return (enhanced_image * 255).astype(np.uint8)\n",
    "\n",
    "def remove_stains(image):\n",
    "    mask = cv2.inRange(image, np.array([0, 0, 0]), np.array([60, 60, 60]))\n",
    "    inpainted_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n",
    "    return inpainted_image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    \n",
    "    denoised = denoise_image(image)\n",
    "    deblurred = deblur_image(denoised)\n",
    "    \n",
    "    contrast_enhanced = enhance_contrast(deblurred)\n",
    "    if len(contrast_enhanced.shape) == 2:\n",
    "        contrast_enhanced = cv2.cvtColor(contrast_enhanced, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    restored_image = remove_stains(contrast_enhanced)\n",
    "    if len(restored_image.shape) == 2:\n",
    "        restored_image = cv2.cvtColor(restored_image, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    restored_image = Image.fromarray(restored_image)\n",
    "    restored_image = ImageEnhance.Contrast(restored_image).enhance(2)\n",
    "    restored_image = ImageEnhance.Brightness(restored_image).enhance(1.5)\n",
    "    \n",
    "    return restored_image\n",
    "\n",
    "def recognize_text(image):\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values, max_new_tokens=50)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "def find_best_orientation(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = preprocess_image(img)\n",
    "\n",
    "    angles = [0, 90, 180, 270]\n",
    "    best_score = 0\n",
    "    best_transformation = None\n",
    "\n",
    "    for angle in angles:\n",
    "        rotated_image = preprocessed_image.rotate(angle, expand=True)\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Angle: {angle}, Readability Score: {readability_score}\")\n",
    "\n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "\n",
    "    flipped_image = preprocessed_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    for angle in angles:\n",
    "        rotated_image = flipped_image.rotate(angle, expand=True)\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Flipped Angle: {angle}, Readability Score: {readability_score}\")\n",
    "\n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "\n",
    "    if best_transformation is None:\n",
    "        best_transformation = preprocessed_image\n",
    "\n",
    "    return best_transformation\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "best_image = find_best_orientation(image_path)\n",
    "if best_image is not None:\n",
    "    best_image.save('dj/best_oriented_image.jpg')\n",
    "    print(\"Best oriented image saved to 'dj/best_oriented_image.jpg'\")\n",
    "else:\n",
    "    print(\"No suitable transformation found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle: 0, Readability Score: 0\n",
      "Angle: 90, Readability Score: 0\n",
      "Angle: 180, Readability Score: 0\n",
      "Angle: 270, Readability Score: 0\n",
      "Flipped Angle: 0, Readability Score: 0\n",
      "Flipped Angle: 90, Readability Score: 0\n",
      "Flipped Angle: 180, Readability Score: 0\n",
      "Flipped Angle: 270, Readability Score: 0\n",
      "Best oriented image saved to 'dj/best_oriented_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Load the pre-trained Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image):\n",
    "    # Resize image to improve OCR accuracy\n",
    "    resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Additional noise removal using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    denoised = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Apply CLAHE to improve contrast\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_image = clahe.apply(denoised)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    enhanced_image_rgb = cv2.cvtColor(enhanced_image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    return enhanced_image_rgb\n",
    "\n",
    "# Function to recognize text using Donut model\n",
    "def recognize_text(image):\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values, max_new_tokens=50)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "# Function to find the best orientation\n",
    "def find_best_orientation(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    preprocessed_image = preprocess_image(img)\n",
    "    angles = [0, 90, 180, 270]\n",
    "    best_score = 0\n",
    "    best_transformation = None\n",
    "    \n",
    "    for angle in angles:\n",
    "        rotated_image = Image.fromarray(np.rot90(preprocessed_image, k=angle // 90))\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Angle: {angle}, Readability Score: {readability_score}\")\n",
    "        \n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "    \n",
    "    flipped_image = np.fliplr(preprocessed_image)\n",
    "    for angle in angles:\n",
    "        rotated_image = Image.fromarray(np.rot90(flipped_image, k=angle // 90))\n",
    "        recognized_text = recognize_text(rotated_image)\n",
    "        readability_score = len(recognized_text)\n",
    "        print(f\"Flipped Angle: {angle}, Readability Score: {readability_score}\")\n",
    "        \n",
    "        if readability_score > best_score:\n",
    "            best_score = readability_score\n",
    "            best_transformation = rotated_image\n",
    "    \n",
    "    if best_transformation is None:\n",
    "        best_transformation = Image.fromarray(preprocessed_image)\n",
    "    \n",
    "    return best_transformation\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "best_image = find_best_orientation(image_path)\n",
    "if best_image is not None:\n",
    "    best_image.save('dj/best_oriented_image.jpg')\n",
    "    print(\"Best oriented image saved to 'dj/best_oriented_image.jpg'\")\n",
    "else:\n",
    "    print(\"No suitable transformation found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Image...\n",
      "Resized Image Shape: (1182, 886, 3)\n",
      "Converted to Grayscale\n",
      "Applied GaussianBlur\n",
      "Applied Adaptive Thresholding\n",
      "Applied Morphological Operations\n",
      "Applied CLAHE\n",
      "Preprocessed image saved to 'dj/preprocessed_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        print(\"Loading Image...\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image_np, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        print(\"Resized Image Shape:\", resized_image.shape)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        print(\"Converted to Grayscale\")\n",
    "\n",
    "        # Apply GaussianBlur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        print(\"Applied GaussianBlur\")\n",
    "\n",
    "        # Apply adaptive thresholding\n",
    "        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "        print(\"Applied Adaptive Thresholding\")\n",
    "\n",
    "        # Additional noise removal using morphological operations\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        denoised = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        print(\"Applied Morphological Operations\")\n",
    "\n",
    "        # Apply CLAHE to improve contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced_image = clahe.apply(denoised)\n",
    "        print(\"Applied CLAHE\")\n",
    "\n",
    "        # Save preprocessed image\n",
    "        preprocessed_image = Image.fromarray(enhanced_image)\n",
    "        preprocessed_image.save('dj/preprocessed_image.jpg')\n",
    "        print(\"Preprocessed image saved to 'dj/preprocessed_image.jpg'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed image saved to dj/preprocessed_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "\n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply GaussianBlur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        \n",
    "        # Apply adaptive thresholding\n",
    "        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "        \n",
    "        # Additional noise removal using morphological operations\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        denoised = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply CLAHE to improve contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced_image = clahe.apply(denoised)\n",
    "        \n",
    "        # Convert back to PIL Image for further enhancements\n",
    "        pil_image = Image.fromarray(enhanced_image)\n",
    "        \n",
    "        # Enhance contrast and brightness\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "        \n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during preprocessing: pic should be 2/3 dimensional. Got 1 dimensions.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "# Define the sharpening kernel\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "# Enhance edges using Canny edge detection\n",
    "def edge_detection(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "\n",
    "# Load and use a pre-trained deep learning model for image enhancement\n",
    "def enhance_image(image_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced_tensor = model(img_tensor)\n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "\n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Sharpen the image\n",
    "        sharpened_image = sharpen_image(resized_image)\n",
    "        \n",
    "        # Enhance edges\n",
    "        edges = edge_detection(sharpened_image)\n",
    "        \n",
    "        # Apply deep learning enhancement\n",
    "        enhanced_image = enhance_image(image_path)\n",
    "\n",
    "        # Convert the enhanced image back to OpenCV format\n",
    "        enhanced_image = cv2.cvtColor(np.array(enhanced_image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Combine the processed images\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.5, edges, 0.5, 0)\n",
    "        combined_image = cv2.addWeighted(combined_image, 0.7, enhanced_image, 0.3, 0)\n",
    "\n",
    "        # Convert back to PIL Image for further enhancements\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Enhance contrast and brightness\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "        \n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during preprocessing: pic should be 2/3 dimensional. Got 1 dimensions.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "# Define the sharpening kernel\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "# Enhance edges using Canny edge detection\n",
    "def edge_detection(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "\n",
    "# Load and use a pre-trained deep learning model for image enhancement\n",
    "def enhance_image(image_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced_tensor = model(img_tensor)\n",
    "    \n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "\n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Sharpen the image\n",
    "        sharpened_image = sharpen_image(resized_image)\n",
    "        \n",
    "        # Enhance edges\n",
    "        edges = edge_detection(sharpened_image)\n",
    "        \n",
    "        # Apply deep learning enhancement\n",
    "        enhanced_image = enhance_image(image_path)\n",
    "\n",
    "        # Convert the enhanced image back to OpenCV format\n",
    "        enhanced_image = cv2.cvtColor(np.array(enhanced_image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Combine the processed images\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.5, edges, 0.5, 0)\n",
    "        combined_image = cv2.addWeighted(combined_image, 0.7, enhanced_image, 0.3, 0)\n",
    "\n",
    "        # Convert back to PIL Image for further enhancements\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Enhance contrast and brightness\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "        \n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (591, 443, 3)\n",
      "Resized image shape: (1182, 886, 3)\n",
      "Sharpened image shape: (1182, 886, 3)\n",
      "Edges image shape: (1182, 886)\n",
      "Error during preprocessing: pic should be 2/3 dimensional. Got 1 dimensions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "# Define the sharpening kernel\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "# Enhance edges using Canny edge detection\n",
    "def edge_detection(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "\n",
    "# Load and use a pre-trained deep learning model for image enhancement\n",
    "def enhance_image(image_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced_tensor = model(img_tensor)\n",
    "    \n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        print(f\"Resized image shape: {resized_image.shape}\")\n",
    "        \n",
    "        # Sharpen the image\n",
    "        sharpened_image = sharpen_image(resized_image)\n",
    "        print(f\"Sharpened image shape: {sharpened_image.shape}\")\n",
    "        \n",
    "        # Enhance edges\n",
    "        edges = edge_detection(sharpened_image)\n",
    "        print(f\"Edges image shape: {edges.shape}\")\n",
    "        \n",
    "        # Apply deep learning enhancement\n",
    "        enhanced_image = enhance_image(image_path)\n",
    "        print(f\"Enhanced image type: {type(enhanced_image)}\")\n",
    "        \n",
    "        # Convert the enhanced image back to OpenCV format\n",
    "        enhanced_image = cv2.cvtColor(np.array(enhanced_image), cv2.COLOR_RGB2BGR)\n",
    "        print(f\"Enhanced image shape: {enhanced_image.shape}\")\n",
    "        \n",
    "        # Combine the processed images\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.5, edges, 0.5, 0)\n",
    "        print(f\"Combined image shape after adding edges: {combined_image.shape}\")\n",
    "        combined_image = cv2.addWeighted(combined_image, 0.7, enhanced_image, 0.3, 0)\n",
    "        print(f\"Combined image shape after adding enhanced image: {combined_image.shape}\")\n",
    "\n",
    "        # Convert back to PIL Image for further enhancements\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "        print(f\"PIL image type: {type(pil_image)}\")\n",
    "        \n",
    "        # Enhance contrast and brightness\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "        \n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (591, 443, 3)\n",
      "Resized image shape: (1182, 886, 3)\n",
      "Sharpened image shape: (1182, 886, 3)\n",
      "Edges image shape: (1182, 886)\n",
      "Edges colored image shape: (1182, 886, 3)\n",
      "Error during preprocessing: pic should be 2/3 dimensional. Got 1 dimensions.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "# Define the sharpening kernel\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "# Enhance edges using Canny edge detection\n",
    "def edge_detection(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "\n",
    "# Load and use a pre-trained deep learning model for image enhancement\n",
    "def enhance_image(image_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced_tensor = model(img_tensor)\n",
    "    \n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "        # Resize image to improve OCR accuracy\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        print(f\"Resized image shape: {resized_image.shape}\")\n",
    "        \n",
    "        # Sharpen the image\n",
    "        sharpened_image = sharpen_image(resized_image)\n",
    "        print(f\"Sharpened image shape: {sharpened_image.shape}\")\n",
    "        \n",
    "        # Enhance edges\n",
    "        edges = edge_detection(sharpened_image)\n",
    "        print(f\"Edges image shape: {edges.shape}\")\n",
    "        \n",
    "        # Convert edges to three channels\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        print(f\"Edges colored image shape: {edges_colored.shape}\")\n",
    "        \n",
    "        # Apply deep learning enhancement\n",
    "        enhanced_image = enhance_image(image_path)\n",
    "        print(f\"Enhanced image type: {type(enhanced_image)}\")\n",
    "        \n",
    "        # Convert the enhanced image back to OpenCV format\n",
    "        enhanced_image = cv2.cvtColor(np.array(enhanced_image), cv2.COLOR_RGB2BGR)\n",
    "        print(f\"Enhanced image shape: {enhanced_image.shape}\")\n",
    "        \n",
    "        # Combine the processed images\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.5, edges_colored, 0.5, 0)\n",
    "        print(f\"Combined image shape after adding edges: {combined_image.shape}\")\n",
    "        combined_image = cv2.addWeighted(combined_image, 0.7, enhanced_image, 0.3, 0)\n",
    "        print(f\"Combined image shape after adding enhanced image: {combined_image.shape}\")\n",
    "\n",
    "        # Convert back to PIL Image for further enhancements\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "        print(f\"PIL image type: {type(pil_image)}\")\n",
    "        \n",
    "        # Enhance contrast and brightness\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "        \n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "\n",
    "\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (591, 443, 3)\n",
      "Resized image shape: (1182, 886, 3)\n",
      "Sharpened image shape: (1182, 886, 3)\n",
      "Edges image shape: (1182, 886)\n",
      "Edges colored image shape: (1182, 886, 3)\n",
      "Error during preprocessing: pic should be 2/3 dimensional. Got 1 dimensions.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened\n",
    "\n",
    "def edge_detection(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges\n",
    "\n",
    "def enhance_image(image_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Identity()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced_tensor = model(img_tensor)\n",
    "    \n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_image(image_path, output_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(\"Could not open or find the image.\")\n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        print(f\"Resized image shape: {resized_image.shape}\")\n",
    "\n",
    "        sharpened_image = sharpen_image(resized_image)\n",
    "        print(f\"Sharpened image shape: {sharpened_image.shape}\")\n",
    "\n",
    "        edges = edge_detection(sharpened_image)\n",
    "        print(f\"Edges image shape: {edges.shape}\")\n",
    "\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        print(f\"Edges colored image shape: {edges_colored.shape}\")\n",
    "\n",
    "        enhanced_image = enhance_image(image_path)\n",
    "        print(f\"Enhanced image type: {type(enhanced_image)}\")\n",
    "\n",
    "        enhanced_image = enhanced_image.resize(resized_image.shape[1::-1])\n",
    "        enhanced_image = cv2.cvtColor(np.array(enhanced_image), cv2.COLOR_RGB2BGR)\n",
    "        print(f\"Enhanced image shape: {enhanced_image.shape}\")\n",
    "\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.5, edges_colored, 0.5, 0)\n",
    "        print(f\"Combined image shape after adding edges: {combined_image.shape}\")\n",
    "        combined_image = cv2.addWeighted(combined_image, 0.7, enhanced_image, 0.3, 0)\n",
    "        print(f\"Combined image shape after adding enhanced image: {combined_image.shape}\")\n",
    "\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "        print(f\"PIL image type: {type(pil_image)}\")\n",
    "\n",
    "        enhancer = ImageEnhance.Contrast(pil_image)\n",
    "        pil_image = enhancer.enhance(2)\n",
    "\n",
    "        enhancer = ImageEnhance.Brightness(pil_image)\n",
    "        pil_image = enhancer.enhance(1.5)\n",
    "\n",
    "        pil_image.save(output_path)\n",
    "        print(f\"Preprocessed image saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "\n",
    "\n",
    "\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "output_path = 'dj/preprocessed_image.jpg'\n",
    "preprocess_image(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (591, 443, 3)\n",
      "Resized image shape: (1182, 886, 3)\n",
      "Sharpened image shape: (1182, 886, 3)\n",
      "Edges image shape: (1182, 886)\n",
      "Edges colored image shape: (1182, 886, 3)\n",
      "Preprocessed image saved to 'dj/preprocessed_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        print(\"Original image shape:\", image.shape)\n",
    "\n",
    "        # Resize image\n",
    "        resized_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "        print(\"Resized image shape:\", resized_image.shape)\n",
    "\n",
    "        # Sharpen the image\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "        sharpened_image = cv2.filter2D(resized_image, -1, kernel)\n",
    "        print(\"Sharpened image shape:\", sharpened_image.shape)\n",
    "\n",
    "        # Edge detection\n",
    "        edges = cv2.Canny(sharpened_image, 100, 200)\n",
    "        print(\"Edges image shape:\", edges.shape)\n",
    "\n",
    "        # Convert edges to 3 channels\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        print(\"Edges colored image shape:\", edges_colored.shape)\n",
    "\n",
    "        # Combine the sharpened image and edges\n",
    "        combined_image = cv2.addWeighted(sharpened_image, 0.8, edges_colored, 0.2, 0)\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Enhance the image contrast and brightness\n",
    "        pil_image = ImageEnhance.Contrast(pil_image).enhance(2)\n",
    "        pil_image = ImageEnhance.Brightness(pil_image).enhance(1.5)\n",
    "\n",
    "        # Save the preprocessed image\n",
    "        preprocessed_path = 'dj/preprocessed_image.jpg'\n",
    "        pil_image.save(preprocessed_path)\n",
    "        print(\"Preprocessed image saved to 'dj/preprocessed_image.jpg'\")\n",
    "\n",
    "        return preprocessed_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during preprocessing:\", e)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "preprocessed_image_path = preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (443, 591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179068/1396399940.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'super_res_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuper_res_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ensure you have the model file here\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m, in \u001b[0;36menhance_image\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m SuperResolutionNet(upscale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Preprocess image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'super_res_model.pth'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# Define the Super-Resolution model\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 3 * (upscale_factor ** 2), kernel_size=3, stride=1, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "def enhance_image(image_path, model_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    print(\"Original image shape:\", image.size)\n",
    "\n",
    "    # Load model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SuperResolutionNet(upscale_factor=3).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess image\n",
    "    image_tensor = ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Enhance image using the model\n",
    "        enhanced_tensor = model(image_tensor)\n",
    "    \n",
    "    enhanced_image = ToPILImage()(enhanced_tensor.squeeze(0).cpu())\n",
    "    print(\"Enhanced image shape:\", enhanced_image.size)\n",
    "\n",
    "    # Convert enhanced image to numpy array for further processing\n",
    "    enhanced_image_np = np.array(enhanced_image)\n",
    "\n",
    "    # Apply sharpening\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened_image = cv2.filter2D(enhanced_image_np, -1, kernel)\n",
    "    print(\"Sharpened image shape:\", sharpened_image.shape)\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(sharpened_image, 100, 200)\n",
    "    print(\"Edges image shape:\", edges.shape)\n",
    "\n",
    "    # Convert edges to 3 channels\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "    print(\"Edges colored image shape:\", edges_colored.shape)\n",
    "\n",
    "    # Combine the sharpened image and edges\n",
    "    combined_image = cv2.addWeighted(sharpened_image, 0.8, edges_colored, 0.2, 0)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Enhance the image contrast and brightness\n",
    "    pil_image = ImageEnhance.Contrast(pil_image).enhance(2)\n",
    "    pil_image = ImageEnhance.Brightness(pil_image).enhance(1.5)\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = 'super_res_model.pth'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181103/1179060273.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path), strict=True)\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '<'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     85\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/super_res_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ensure you have the model file here\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m, in \u001b[0;36menhance_image\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m RRDBNet(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m23\u001b[39m, gc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 70\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     73\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1338\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import functools\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        RRDB_block_f = functools.partial(RRDB, nf=nf, gc=gc)\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
    "        self.RRDB_trunk = self.make_layer(RRDB_block_f, nb)\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "\n",
    "    def make_layer(self, block, n_layers):\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(block())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.conv_first(x)\n",
    "        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n",
    "        fea = fea + trunk\n",
    "        fea = self.upconv1(nn.functional.interpolate(fea, scale_factor=2, mode='nearest'))\n",
    "        fea = self.upconv2(nn.functional.interpolate(fea, scale_factor=2, mode='nearest'))\n",
    "        out = self.conv_last(self.HRconv(fea))\n",
    "        return out\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, nf, gc=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.RDB1(x)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "def enhance_image(image_path, model_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RRDBNet(3, 3, 64, 23, gc=32).to(device)\n",
    "    model.load_state_dict(torch.load(model_path), strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(image_tensor)\n",
    "\n",
    "    sr_image = ToPILImage()(sr_tensor.squeeze().cpu())\n",
    "    return sr_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = '/dj/super_res_model.pth'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181103/3366680254.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "could not find MARK",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     46\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 47\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_dip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36menhance_image_with_dip\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m DIPModel()\n\u001b[0;32m---> 34\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Enhance image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1338\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: could not find MARK"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load pre-trained DIP model\n",
    "model_url = 'https://github.com/DmitryUlyanov/deep-image-prior/releases/download/1.0/DIP_model.pth'\n",
    "model_path = '/dj/DIP_model.pth'\n",
    "response = requests.get(model_url)\n",
    "with open(model_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "class DIPModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DIPModel, self).__init__()\n",
    "        # Define model architecture here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass here\n",
    "        return x\n",
    "\n",
    "def enhance_image_with_dip(image_path, model_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Load model\n",
    "    model = DIPModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Enhance image\n",
    "    with torch.no_grad():\n",
    "        enhanced_image_tensor = model(image_tensor)\n",
    "\n",
    "    # Convert tensor back to image\n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_image_tensor.squeeze(0))\n",
    "    return enhanced_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "enhanced_image = enhance_image_with_dip(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@1164.452] global net_impl.cpp:1162 getLayerShapesRecursively OPENCV/DNN: [Convolution]:(conv1): getMemoryShapes() throws exception. inputs=1 outputs=0/1 blobs=1\n",
      "[ERROR:0@1164.452] global net_impl.cpp:1168 getLayerShapesRecursively     input[0] = [ 1 3 591 443 ]\n",
      "[ERROR:0@1164.452] global net_impl.cpp:1176 getLayerShapesRecursively     blobs[0] = CV_32FC1 [ 56 1 5 5 ]\n",
      "[ERROR:0@1164.452] global net_impl.cpp:1178 getLayerShapesRecursively Exception message: OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/FSRCNN_x4.pb\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ensure you have the model file here\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_fsrcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36menhance_image_with_fsrcnn\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform the super-resolution\u001b[39;00m\n\u001b[1;32m     15\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[1;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(output)\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def enhance_image_with_fsrcnn(image_path, model_path):\n",
    "    # Load the pre-trained FSRCNN model\n",
    "    net = cv2.dnn.readNetFromTensorflow(model_path)\n",
    "    \n",
    "    # Read and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, c = image.shape\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(w, h), mean=(0, 0, 0), swapRB=False, crop=False)\n",
    "    \n",
    "    # Perform the super-resolution\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "    \n",
    "    # Post-process the output\n",
    "    output = np.squeeze(output).transpose((1, 2, 0))\n",
    "    output = np.clip(output, 0, 255).astype('uint8')\n",
    "    \n",
    "    return Image.fromarray(output)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = '/dj/FSRCNN_x4.pb'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image_with_fsrcnn(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /data/ephemeral/home/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|| 528M/528M [00:08<00:00, 68.7MB/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should not have > 4 channels. Got 512 channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     35\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_vgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36menhance_image_with_vgg16\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m output_tensor\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m output_image \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_image\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torchvision/transforms/functional.py:277\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should not have > 4 channels. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m channels.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m npimg \u001b[38;5;241m=\u001b[39m pic\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(npimg\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: pic should not have > 4 channels. Got 512 channels."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to enhance image using VGG16 pre-trained model for super resolution\n",
    "def enhance_image_with_vgg16(image_path):\n",
    "    # Load pre-trained VGG16 model\n",
    "    model = vgg16(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "\n",
    "    # Perform super-resolution\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model.features(input_tensor)\n",
    "        output_tensor = torch.nn.functional.interpolate(output_tensor, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        output_tensor = output_tensor.clamp(0, 1)\n",
    "\n",
    "    # Post-process the output\n",
    "    output_image = transforms.ToPILImage()(output_tensor.squeeze())\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "enhanced_image = enhance_image_with_vgg16(image_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181103/633738435.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/your/srcnn_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     45\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/your/srcnn_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ensure you have the model file here\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_srcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36menhance_image_with_srcnn\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance_image_with_srcnn\u001b[39m(image_path, model_path):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Load pre-trained SRCNN model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m SRCNN()\n\u001b[0;32m---> 27\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Load and preprocess image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/srcnn_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the SRCNN model\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Function to enhance image using SRCNN\n",
    "def enhance_image_with_srcnn(image_path, model_path):\n",
    "    # Load pre-trained SRCNN model\n",
    "    model = SRCNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = ToTensor()(image).unsqueeze(0)\n",
    "\n",
    "    # Perform super-resolution\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Post-process the output\n",
    "    output_image = ToPILImage()(output.squeeze())\n",
    "\n",
    "    return output_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = 'path/to/your/srcnn_model.pth'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image_with_srcnn(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrrdbnet_arch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RRDBNet\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrealesrgan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RealESRGANer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance_image_with_realesrgan\u001b[39m(image_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/realesrgan/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/realesrgan/data/__init__.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrealesrgan.data.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/realesrgan/data/__init__.py:10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrealesrgan.data.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/realesrgan/data/realesrgan_dataset.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdegradations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m circular_lowpass_kernel, random_mixed_kernels\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augment\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileClient, get_root_logger, imfrombytes, img2tensor\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://github.com/xinntao/BasicSR\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasicsr.data.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/__init__.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasicsr.data.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/realesrgan_dataset.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m data\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdegradations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m circular_lowpass_kernel, random_mixed_kernels\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augment\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileClient, get_root_logger, imfrombytes, img2tensor\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/degradations.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m special\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rgb_to_grayscale\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --------------------------- blur kernels --------------------------- #\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# --------------------------- util functions --------------------------- #\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigma_matrix2\u001b[39m(sig_x, sig_y, theta):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "\n",
    "def enhance_image_with_realesrgan(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Unable to load image at {image_path}\")\n",
    "\n",
    "    # Initialize the Real-ESRGAN model\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "    model_path = '/dj/Real-ESRGAN/weights/RealESRGAN_x4.pth'\n",
    "    realesrgan = RealESRGANer(scale=4, model_path=model_path, model=model, tile=0, tile_pad=10, pre_pad=0, half=True)\n",
    "\n",
    "    # Enhance image\n",
    "    enhanced_image, _ = realesrgan.enhance(image)\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "enhanced_image = enhance_image_with_realesrgan(image_path)\n",
    "cv2.imwrite('/dj/enhanced_image.png', enhanced_image)\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/ephemeral/home/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot find callable srresnet in hubconf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     34\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 35\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_super_resolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m, in \u001b[0;36menhance_image_with_super_resolution\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance_image_with_super_resolution\u001b[39m(image_path):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the pre-trained model from torchvision\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch/vision:v0.10.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrresnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:570\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    567\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    568\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 570\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:598\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m    596\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m--> 598\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[43m_load_entry_from_hubconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:349\u001b[0m, in \u001b[0;36m_load_entry_from_hubconf\u001b[0;34m(m, model)\u001b[0m\n\u001b[1;32m    346\u001b[0m func \u001b[38;5;241m=\u001b[39m _load_attr_from_module(m, model)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in hubconf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot find callable srresnet in hubconf"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Function to enhance the image using a pre-trained super-resolution model\n",
    "def enhance_image_with_super_resolution(image_path):\n",
    "    # Load the pre-trained model from torchvision\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'srresnet', pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    # Enhance the image\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(input_tensor)\n",
    "    \n",
    "    # Post-process the output image\n",
    "    output_tensor = output_tensor.squeeze(0).clamp(0, 1)\n",
    "    output_image = transforms.ToPILImage()(output_tensor)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "enhanced_image = enhance_image_with_super_resolution(image_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrrdbnet_arch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RRDBNet\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrealesrgan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RealESRGANer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://github.com/xinntao/BasicSR\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasicsr.data.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/__init__.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasicsr.data.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/realesrgan_dataset.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m data\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdegradations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m circular_lowpass_kernel, random_mixed_kernels\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augment\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileClient, get_root_logger, imfrombytes, img2tensor\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/basicsr/data/degradations.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m special\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rgb_to_grayscale\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --------------------------- blur kernels --------------------------- #\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# --------------------------- util functions --------------------------- #\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigma_matrix2\u001b[39m(sig_x, sig_y, theta):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from PIL import Image\n",
    "\n",
    "def enhance_image_with_realesrgan(image_path, model_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Initialize model\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=4,\n",
    "        model_path=model_path,\n",
    "        model=model,\n",
    "        tile=0,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=False,\n",
    "    )\n",
    "\n",
    "    # Enhance image\n",
    "    enhanced_image, _ = upsampler.enhance(image, outscale=4)\n",
    "\n",
    "    # Convert the enhanced image to PIL format for saving\n",
    "    enhanced_image = Image.fromarray(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = 'RealESRGAN_x4.pth'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image_with_realesrgan(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@0.084] global net_impl.cpp:1162 getLayerShapesRecursively OPENCV/DNN: [Convolution]:(conv1): getMemoryShapes() throws exception. inputs=1 outputs=0/1 blobs=1\n",
      "[ERROR:0@0.084] global net_impl.cpp:1168 getLayerShapesRecursively     input[0] = [ 1 3 591 443 ]\n",
      "[ERROR:0@0.084] global net_impl.cpp:1176 getLayerShapesRecursively     blobs[0] = CV_32FC1 [ 56 1 5 5 ]\n",
      "[ERROR:0@0.084] global net_impl.cpp:1178 getLayerShapesRecursively Exception message: OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     31\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/FSRCNN_x4.pb\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ensure you have the model file here\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_espcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36menhance_image_with_espcn\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Perform super-resolution\u001b[39;00m\n\u001b[1;32m     16\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(output)\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /home/conda/feedstock_root/build_artifacts/libopencv_1721302265921/work/modules/dnn/src/layers/convolution_layer.cpp:397: error: (-215:Assertion failed) ngroups > 0 && inpCn % ngroups == 0 && outCn % ngroups == 0 in function 'getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def enhance_image_with_espcn(image_path, model_path):\n",
    "    # Load the pre-trained ESPCN model\n",
    "    net = cv2.dnn.readNetFromTensorflow(model_path)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(image.shape[1], image.shape[0]), mean=(0, 0, 0), swapRB=False, crop=False)\n",
    "    \n",
    "    # Perform super-resolution\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "    \n",
    "    # Post-process the output\n",
    "    output = np.squeeze(output).transpose((1, 2, 0))\n",
    "    output = np.clip(output * 255.0, 0, 255).astype(np.uint8)\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    enhanced_image = Image.fromarray(output)\n",
    "    \n",
    "    return enhanced_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "model_path = '/dj/FSRCNN_x4.pb'  # Ensure you have the model file here\n",
    "enhanced_image = enhance_image_with_espcn(image_path, model_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/ephemeral/home/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot find callable esrgan in hubconf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     25\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdj/data/test/0a4f2decf34d3bff.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m enhanced_image \u001b[38;5;241m=\u001b[39m \u001b[43menhance_image_with_esrgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m enhanced_image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced image saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dj/enhanced_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36menhance_image_with_esrgan\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance_image_with_esrgan\u001b[39m(image_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the pre-trained ESRGAN model from PyTorch Hub\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch/vision:v0.10.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mesrgan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:570\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    567\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    568\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 570\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:598\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m    596\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m--> 598\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[43m_load_entry_from_hubconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/ocr_env2/lib/python3.10/site-packages/torch/hub.py:349\u001b[0m, in \u001b[0;36m_load_entry_from_hubconf\u001b[0;34m(m, model)\u001b[0m\n\u001b[1;32m    346\u001b[0m func \u001b[38;5;241m=\u001b[39m _load_attr_from_module(m, model)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in hubconf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot find callable esrgan in hubconf"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "def enhance_image_with_esrgan(image_path):\n",
    "    # Load the pre-trained ESRGAN model from PyTorch Hub\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'esrgan', pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = ToTensor()(image).unsqueeze(0)\n",
    "\n",
    "    # Perform super-resolution\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Post-process the output\n",
    "    output_image = ToPILImage()(output.squeeze())\n",
    "\n",
    "    return output_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'dj/data/test/0a4f2decf34d3bff.jpg'\n",
    "enhanced_image = enhance_image_with_esrgan(image_path)\n",
    "enhanced_image.save('/dj/enhanced_image.png')\n",
    "print(\"Enhanced image saved to '/dj/enhanced_image.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
