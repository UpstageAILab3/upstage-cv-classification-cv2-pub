{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìú Î¨∏ÏÑú ÌÉÄÏûÖ Î∂ÑÎ•ò ÎåÄÌöå\n",
    "\n",
    "> - kimkihong / helpotcreator@gmail.com / Upstage AI Lab 3Í∏∞\n",
    "> - 2024.07.30.Ìôî 10:00 ~ 2024.08.11.Ïùº 19:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from glob import glob \n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations import ImageOnlyTransform\n",
    "from augraphy import *\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import cv2\n",
    "import platform\n",
    "\n",
    "os_name = platform.system()\n",
    "if os_name == 'Windows':\n",
    "    PRE_PATH = ''\n",
    "elif os_name == 'Linux':\n",
    "    PRE_PATH = '/kkh/'\n",
    "elif os_name == 'Darwin': # Îß•\n",
    "    PRE_PATH = '/kkh/'\n",
    "\n",
    "TRAIN_KR_IMAGE_PATH = PRE_PATH + 'data/train_kr'\n",
    "TRAIN_KR_AUG_IMAGE_PATH = PRE_PATH + 'data/train_kr_aug' # Ï¶ùÍ∞ïÌïú Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Îã¥ÏùÑ Ìè¥ÎçîÎ™Ö ÎØ∏Î¶¨ ÏßÄÏ†ï\n",
    "META_KR_CSV_PATH = PRE_PATH + 'data/meta_kr.csv'\n",
    "META_KR_DF = pd.read_csv(META_KR_CSV_PATH)\n",
    "TRAIN_KR_CSV_PATH = PRE_PATH + 'data/train_kr.csv'\n",
    "TRAIN_KR_DF = pd.read_csv(TRAIN_KR_CSV_PATH)\n",
    "\n",
    "COUNT_PATCH = 2\n",
    "ROTATE_ANGLE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ï¶ùÍ∞ïÌïú Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Îã¥ÏùÑ Ìè¥ÎçîÎ•º ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created: /kkh/data/train_kr_aug\n"
     ]
    }
   ],
   "source": [
    "# train_kr_aug_image_path Ìè¥Îçî ÏÉùÏÑ±\n",
    "def create_directory_with_backup(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            backup_path = path + '_backup'\n",
    "            os.rename(path, backup_path)\n",
    "            print(f\"Existing folder renamed to: {backup_path}\")\n",
    "        os.makedirs(path)\n",
    "        print(f\"Folder created: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "create_directory_with_backup(TRAIN_KR_AUG_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, csv, directory, target_size=(380, 380)):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.directory = directory\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.directory, name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image {name} not found in the directory {self.directory}\")\n",
    "        \n",
    "        img = Image.open(img_path).resize(self.target_size)\n",
    "        img = np.array(img)\n",
    "        return img, target, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patching Ï†ÅÏö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1570/1570 [00:07<00:00, 218.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1570/1570 [00:21<00:00, 71.58it/s]\n"
     ]
    }
   ],
   "source": [
    "class PatchAugmentation:\n",
    "    def __init__(self, max_patch_size=(32, 32)):\n",
    "        self.max_patch_size = max_patch_size\n",
    "\n",
    "    def __call__(self, img1, label1, img2, label2):\n",
    "        h, w, _ = img1.shape\n",
    "        ph, pw = min(self.max_patch_size[0], h), min(self.max_patch_size[1], w)\n",
    "        x, y = np.random.randint(0, w - pw + 1), np.random.randint(0, h - ph + 1)\n",
    "        img1[y:y + ph, x:x + pw] = img2[y:y + ph, x:x + pw]\n",
    "        combined_label = label1  # Keep the label of the base image\n",
    "        return img1, combined_label\n",
    "\n",
    "# Load dataset\n",
    "dataset = ImageDataset(TRAIN_KR_CSV_PATH, TRAIN_KR_IMAGE_PATH)\n",
    "patch_augmenter = PatchAugmentation(max_patch_size=(32, 32))\n",
    "\n",
    "patch_csv_data = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    img, label, name = dataset[i]\n",
    "    patch_csv_data.append([name, label])\n",
    "    \n",
    "    # Save original image to augmented directory\n",
    "    original_img_path = os.path.join(TRAIN_KR_IMAGE_PATH, name)\n",
    "    new_img_path = os.path.join(TRAIN_KR_AUG_IMAGE_PATH, name)\n",
    "    shutil.copy(original_img_path, new_img_path)\n",
    "\n",
    "# Augmentation loop\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    img1, label1, name1 = dataset[i]\n",
    "    \n",
    "    # Select a random image2\n",
    "    idx2 = np.random.randint(0, len(dataset))\n",
    "    img2, label2, name2 = dataset[idx2]\n",
    "    img_1_2, label_1_2 = patch_augmenter(img1, label1, img2, label2)\n",
    "\n",
    "    # Select a random image3\n",
    "    if COUNT_PATCH >= 2:\n",
    "        idx3 = np.random.randint(0, len(dataset))\n",
    "        img3, label3, name3 = dataset[idx3]\n",
    "        img_1_2, label_1_2 = patch_augmenter(img_1_2, label_1_2, img3, label3)\n",
    "    \n",
    "    # Save augmented image\n",
    "    new_name = f\"patch_{name1}\"\n",
    "    augmented_img_pil = Image.fromarray(img_1_2.astype(np.uint8))\n",
    "    augmented_img_pil.save(os.path.join(TRAIN_KR_AUG_IMAGE_PATH, new_name))\n",
    "    \n",
    "    # Append to new CSV data\n",
    "    patch_csv_data.append([new_name, label_1_2])\n",
    "\n",
    "patch_csv_df = pd.DataFrame(patch_csv_data, columns=['ID', 'target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patching ÎÇ¥Ïö©ÏùÑ train_kr_aug.csv ÌååÏùºÏóêÎèÑ Ï∂îÍ∞ÄÌïúÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_KR_AUG_DF = pd.concat([patch_csv_df])\n",
    "TRAIN_KR_AUG_CSV_PATH = PRE_PATH + 'data/train_kr_aug.csv'\n",
    "\n",
    "if os.path.exists(TRAIN_KR_AUG_CSV_PATH):\n",
    "    backup_path = TRAIN_KR_AUG_CSV_PATH.replace('.csv', '_backup.csv')\n",
    "    os.rename(TRAIN_KR_AUG_CSV_PATH, backup_path)\n",
    "TRAIN_KR_AUG_DF.to_csv(TRAIN_KR_AUG_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## albumentations ÏÑ∏ÌåÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuple(param, low=None):\n",
    "    if isinstance(param, (list, tuple)):\n",
    "        return tuple(param)\n",
    "    return (low, param) if low is not None else (param, param)\n",
    "\n",
    "class SafeRotate(A.DualTransform):\n",
    "    def __init__(self, limit, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], always_apply=False, p=1):\n",
    "        super(SafeRotate, self).__init__(always_apply, p)\n",
    "        self.limit = to_tuple(limit)\n",
    "        self.interpolation = interpolation\n",
    "        self.border_mode = border_mode\n",
    "        self.value = value\n",
    "\n",
    "    def apply(self, img, angle=0, **params):\n",
    "        h, w = img.shape[:2]\n",
    "        # Calculate the center and rotation matrix\n",
    "        center = (w // 2, h // 2)\n",
    "        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # Calculate the sine and cosine (absolute values)\n",
    "        abs_cos = abs(matrix[0, 0])\n",
    "        abs_sin = abs(matrix[0, 1])\n",
    "\n",
    "        # Find the new width and height bounds\n",
    "        bound_w = int(h * abs_sin + w * abs_cos)\n",
    "        bound_h = int(h * abs_cos + w * abs_sin)\n",
    "\n",
    "        # Adjust the rotation matrix to consider the translation\n",
    "        matrix[0, 2] += bound_w / 2 - center[0]\n",
    "        matrix[1, 2] += bound_h / 2 - center[1]\n",
    "\n",
    "        # Perform the actual rotation and return the image\n",
    "        return cv2.warpAffine(img, matrix, (bound_w, bound_h), flags=self.interpolation, borderMode=self.border_mode, borderValue=self.value)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {'angle': np.random.uniform(self.limit[0], self.limit[1])}\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('limit', 'interpolation', 'border_mode', 'value')\n",
    "\n",
    "class CenterCropWithAspectRatio(A.DualTransform):\n",
    "    def __init__(self, always_apply=False, p=1.0):\n",
    "        super(CenterCropWithAspectRatio, self).__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        h, w = img.shape[:2]\n",
    "        crop_size = int(0.7 * max(h, w))\n",
    "        top = (h - crop_size) // 2\n",
    "        left = (w - crop_size) // 2\n",
    "\n",
    "        # Ensure crop coordinates are within bounds\n",
    "        top = max(top, 0)\n",
    "        left = max(left, 0)\n",
    "        bottom = min(top + crop_size, h)\n",
    "        right = min(left + crop_size, w)\n",
    "\n",
    "        cropped_img = img[top:bottom, left:right]\n",
    "        \n",
    "        # Ensure the result is a numpy array\n",
    "        return np.array(cropped_img)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_aug_types = []\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# ÌöåÏ†ÑÎßå Ï†ÅÏö©\n",
    "def create_rotations():\n",
    "    rotations = {}\n",
    "    for angle in range(0, 360, ROTATE_ANGLE):\n",
    "        key = f\"r{angle:03d}_\"\n",
    "        rotations[key] = A.Compose([\n",
    "            SafeRotate(limit=(angle, angle), border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=1),\n",
    "            CenterCropWithAspectRatio(),\n",
    "            # A.Rotate(limit=(angle, angle), border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=1),\n",
    "        ])\n",
    "    return rotations\n",
    "\n",
    "base_aug_types.extend(create_rotations().items())\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# ÌöåÏ†Ñ + Í∞ÄÏö∞ÏãúÏïà ÎÖ∏Ïù¥Ï¶à\n",
    "def create_rotations_transforms(noise_value_min, noise_value_max):\n",
    "    transforms = {}\n",
    "    for angle in range(0, 360, ROTATE_ANGLE):\n",
    "        key = f\"hf_r{angle:03d}_n{noise_value_min}~{noise_value_max}_\"\n",
    "        transforms[key] = A.Compose([\n",
    "            A.HorizontalFlip(p=1),\n",
    "            SafeRotate(limit=(angle, angle), border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=1),\n",
    "            CenterCropWithAspectRatio(),\n",
    "            # A.Rotate(limit=(angle, angle), border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=1),\n",
    "            A.GaussNoise(var_limit=(noise_value_min, noise_value_max), mean=0, p=1)\n",
    "        ])\n",
    "    return transforms\n",
    "\n",
    "# Í∞ÄÏö∞ÏãúÏïà ÎÖ∏Ïù¥Ï¶à Î≤îÏúÑ\n",
    "noise_value_sets = [\n",
    "    (500, 500),\n",
    "    (2000, 2000)\n",
    "]\n",
    "\n",
    "for noise_value_min, noise_value_max in noise_value_sets:\n",
    "    base_aug_types.extend(create_rotations_transforms(noise_value_min, noise_value_max).items())\n",
    "\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## albumentations Ïù¥Ïö©Ìï¥ÏÑú, Ïù¥ÎØ∏ÏßÄ Ï¶ùÍ∞ï ÏßÑÌñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image augmentation:   0%|          | 0/3140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image augmentation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3140/3140 [17:36<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Îπà Î¶¨Ïä§Ìä∏Î•º Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§. Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÏùò IDÏôÄ ÌÉÄÍ≤ü Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìï† Í≤ÉÏûÖÎãàÎã§.\n",
    "ids = []\n",
    "targets = []\n",
    " \n",
    "# `train_kr_df` DataFrameÏùò Í∞Å ÌñâÏóê ÎåÄÌï¥ Î∞òÎ≥µÌï©ÎãàÎã§.\n",
    "# `itertuples()`Îäî DataFrameÏùÑ ÌäúÌîå ÌòïÌÉúÎ°ú Î∞òÎ≥µÌï† Ïàò ÏûàÍ≤å Ìï¥Ï§çÎãàÎã§.\n",
    "for index, ID, target in tqdm(TRAIN_KR_AUG_DF.itertuples(), total=TRAIN_KR_AUG_DF.shape[0], desc='Image augmentation'):\n",
    "    image_path = os.path.join(TRAIN_KR_AUG_IMAGE_PATH, ID)\n",
    "    image = np.array(Image.open(image_path))\n",
    "    \n",
    "    # `base_aug_types`Ïóê Ï†ïÏùòÎêú Í∞Å Î≥ÄÌôòÏóê ÎåÄÌï¥ Î∞òÎ≥µÌï©ÎãàÎã§.\n",
    "    for prefix, aug_function in base_aug_types:\n",
    "        # Î≥ÄÌôò Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄÎ•º Î≥ÄÌôòÌï©ÎãàÎã§.\n",
    "        transformed_image = aug_function(image=image)['image']\n",
    "        new_ID = prefix + ID\n",
    "        ids.append(new_ID)\n",
    "        targets.append(target)\n",
    "        Image.fromarray(transformed_image).save(os.path.join(TRAIN_KR_AUG_IMAGE_PATH, new_ID))\n",
    "\n",
    "    # Ïù∏Îç±Ïä§Í∞Ä 100 Ïù∏ Í≤ΩÏö∞, ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌëúÏãúÌï©ÎãàÎã§.\n",
    "    if index == 100:\n",
    "        # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌëúÏãú\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(10, 10, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÎì§ ÌëúÏãú\n",
    "        for i, (prefix, aug_function) in enumerate(base_aug_types):\n",
    "            # Î≥ÄÌôò Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄÎ•º Î≥ÄÌôòÌï©ÎãàÎã§.\n",
    "            transformed_image = aug_function(image=image)['image']\n",
    "            \n",
    "            plt.subplot(10, 10, i + 2)\n",
    "            plt.title(prefix)\n",
    "            plt.imshow(transformed_image)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Î≥ÄÌôòÎêú Ïù¥ÎØ∏ÏßÄÏùò IDÏôÄ ÌÉÄÍ≤ü Ï†ïÎ≥¥Î•º Îã¥ÏùÄ DataFrameÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "aug_data = {'ID': ids, 'target': targets}\n",
    "aug_data_df = pd.DataFrame(aug_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏóëÏÖÄ ÌååÏùºÏóê Ï¶ùÍ∞ïÌïú ÎÇ¥Ïö©ÎèÑ ÎÑ£ÎäîÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.concat([TRAIN_KR_AUG_DF, aug_data_df], ignore_index=True)\n",
    "updated_df.to_csv(TRAIN_KR_AUG_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏõêÎ≥∏Í≥º Ï¶ùÍ∞ïÍπåÏßÄÌïú ÎÇ¥Ïö©Îì§Ïùò Í∞úÏàòÎ•º ÎπÑÍµêÌïúÎã§.\n",
    "\n",
    "- /kkh/data/train_kr          ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "- /kkh/data/train_kr.csv      ÏõêÎ≥∏ ÏóëÏÖÄ ÌååÏùº\n",
    "- /kkh/data/train_kr_aug      Ï¶ùÍ∞ï + ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "- /kkh/data/train_kr_aug.csv  Ï¶ùÍ∞ï + ÏõêÎ≥∏ ÏóëÏÖÄ ÌååÏùº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kkh/data/train_kr contains 1570 images.\n",
      "/kkh/data/train_kr.csv contains 1570 image records.\n",
      "=================================\n",
      "/kkh/data/train_kr_aug contains 116180 images.\n",
      "/kkh/data/train_kr_aug.csv contains 116180 image records.\n"
     ]
    }
   ],
   "source": [
    "# Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÌôïÏù∏(ls -1 . | wc -l)\n",
    "\n",
    "# ÏõêÎ≥∏Îßå\n",
    "image_count = len([f for f in os.listdir(TRAIN_KR_IMAGE_PATH) if os.path.isfile(os.path.join(TRAIN_KR_IMAGE_PATH, f))])\n",
    "print(f\"{TRAIN_KR_IMAGE_PATH} contains {image_count} images.\")\n",
    "# CSV ÌååÏùº ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÌôïÏù∏\n",
    "df = pd.read_csv(TRAIN_KR_CSV_PATH)\n",
    "csv_image_count = df.shape[0]\n",
    "print(f\"{TRAIN_KR_CSV_PATH} contains {csv_image_count} image records.\")\n",
    "\n",
    "print(f\"=================================\")\n",
    "\n",
    "# Ï¶ùÍ∞ï + ÏõêÎ≥∏\n",
    "image_count = len([f for f in os.listdir(TRAIN_KR_AUG_IMAGE_PATH) if os.path.isfile(os.path.join(TRAIN_KR_AUG_IMAGE_PATH, f))])\n",
    "print(f\"{TRAIN_KR_AUG_IMAGE_PATH} contains {image_count} images.\")\n",
    "# CSV ÌååÏùº ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÌôïÏù∏\n",
    "df = pd.read_csv(TRAIN_KR_AUG_CSV_PATH)\n",
    "csv_image_count = df.shape[0]\n",
    "print(f\"{TRAIN_KR_AUG_CSV_PATH} contains {csv_image_count} image records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ïù¥ÏÉÅÏπò Îç∞Ïù¥ÌÑ∞Îì§ Ï†ïÏ†úÌïúÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV ÌååÏùºÏùÑ ÏùΩÏñ¥ÏòµÎãàÎã§.\n",
    "df = pd.read_csv(TRAIN_KR_AUG_CSV_PATH)\n",
    "\n",
    "# 45f0d2dfc7e47c03_ÏûÖÌá¥Ïõê ÌôïÏù∏ÏÑú --> ÌÜµÏõêÏßÑÎ£å ÌôïÏù∏ÏÑú --> 7\n",
    "# aec62dced7af97cd_ÏûÖÌá¥Ïõê ÌôïÏù∏ÏÑú --> ÏÜåÍ≤¨ÏÑú --> 14\n",
    "# 8646f2c3280a4f49_ÌÜµÏõêÏßÑÎ£å ÌôïÏù∏ÏÑú --> ÏûÖÌá¥Ïõê ÌôïÏù∏ÏÑú --> 3\n",
    "# 1ec14a14bbe633db_ÏÜåÍ≤¨ÏÑú --> ÌÜµÏõêÏßÑÎ£å ÌôïÏù∏ÏÑú --> 7\n",
    "# 7100c5c67aecadc5_ÏûÖÌá¥Ïõê ÌôïÏù∏ÏÑú --> ÌÜµÏõêÏßÑÎ£å ÌôïÏù∏ÏÑú --> 7\n",
    "# c5182ab809478f12_ÏßÑÎã®ÏÑú --> ÏÜåÍ≤¨ÏÑú --> 14\n",
    "# 38d1796b6ad99ddd_ÏïΩÏ†úÎπÑ ÏòÅÏàòÏ¶ù --> ÏßÑÎ£åÎπÑ ÎÇ©ÏûÖ ÌôïÏù∏ÏÑú --> 10\n",
    "# 0583254a73b48ece_ÏïΩÏ†úÎπÑ ÏòÅÏàòÏ¶ù --> ÏßÑÎ£åÎπÑ ÎÇ©ÏûÖ ÌôïÏù∏ÏÑú --> 10\n",
    "\n",
    "# Ï°∞Í±¥Ïóê Îî∞Îùº ÌÉÄÍ≤ü Í∞íÏùÑ Î≥ÄÍ≤ΩÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def update_target(row):\n",
    "    if \"45f0d2dfc7e47c03\" in row['ID']: return 7\n",
    "    elif \"aec62dced7af97cd\" in row['ID']: return 14\n",
    "    elif \"8646f2c3280a4f49\" in row['ID']: return 3\n",
    "    elif \"1ec14a14bbe633db\" in row['ID']: return 7\n",
    "    elif \"7100c5c67aecadc5\" in row['ID']: return 7\n",
    "    elif \"c5182ab809478f12\" in row['ID']: return 14\n",
    "    elif \"38d1796b6ad99ddd\" in row['ID']: return 10\n",
    "    elif \"0583254a73b48ece\" in row['ID']: return 10\n",
    "    else: return row['target']\n",
    "\n",
    "df['target'] = df.apply(update_target, axis=1)\n",
    "df.to_csv(TRAIN_KR_AUG_CSV_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
