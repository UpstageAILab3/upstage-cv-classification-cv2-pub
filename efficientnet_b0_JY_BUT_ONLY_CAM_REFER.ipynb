{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/root/data/train.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4525: 100%|██████████| 40/40 [00:09<00:00,  4.26it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train train_loss: 1.7131\n",
      "Train train_acc: 0.5072\n",
      "Train train_f1: 0.4728\n",
      "Val val_loss: 1.0926\n",
      "Val val_acc: 0.6847\n",
      "Val val_f1: 0.6458\n",
      "Val epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.8498: 100%|██████████| 40/40 [00:09<00:00,  4.23it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train train_loss: 0.5954\n",
      "Train train_acc: 0.7970\n",
      "Train train_f1: 0.7840\n",
      "Val val_loss: 0.8991\n",
      "Val val_acc: 0.7357\n",
      "Val val_f1: 0.7143\n",
      "Val epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0682: 100%|██████████| 40/40 [00:09<00:00,  4.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train train_loss: 0.2953\n",
      "Train train_acc: 0.9045\n",
      "Train train_f1: 0.8986\n",
      "Val val_loss: 1.0659\n",
      "Val val_acc: 0.7197\n",
      "Val val_f1: 0.6878\n",
      "Val epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.4297: 100%|██████████| 40/40 [00:09<00:00,  4.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train train_loss: 0.1620\n",
      "Train train_acc: 0.9419\n",
      "Train train_f1: 0.9347\n",
      "Val val_loss: 1.1029\n",
      "Val val_acc: 0.7293\n",
      "Val val_f1: 0.6997\n",
      "Val epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0337: 100%|██████████| 40/40 [00:09<00:00,  4.14it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train train_loss: 0.1419\n",
      "Train train_acc: 0.9602\n",
      "Train train_f1: 0.9589\n",
      "Val val_loss: 1.1420\n",
      "Val val_acc: 0.7325\n",
      "Val val_f1: 0.6867\n",
      "Val epoch: 4.0000\n",
      "\n",
      "Early stopping\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3293: 100%|██████████| 40/40 [00:09<00:00,  4.12it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train train_loss: 1.8521\n",
      "Train train_acc: 0.4658\n",
      "Train train_f1: 0.4374\n",
      "Val val_loss: 1.2334\n",
      "Val val_acc: 0.6146\n",
      "Val val_f1: 0.5620\n",
      "Val epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.7780: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train train_loss: 0.5658\n",
      "Train train_acc: 0.8209\n",
      "Train train_f1: 0.8125\n",
      "Val val_loss: 1.2314\n",
      "Val val_acc: 0.6561\n",
      "Val val_f1: 0.6099\n",
      "Val epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0180: 100%|██████████| 40/40 [00:09<00:00,  4.14it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train train_loss: 0.2873\n",
      "Train train_acc: 0.8989\n",
      "Train train_f1: 0.8881\n",
      "Val val_loss: 1.2822\n",
      "Val val_acc: 0.6338\n",
      "Val val_f1: 0.6244\n",
      "Val epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.3163: 100%|██████████| 40/40 [00:09<00:00,  4.28it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train train_loss: 0.1695\n",
      "Train train_acc: 0.9467\n",
      "Train train_f1: 0.9416\n",
      "Val val_loss: 1.0572\n",
      "Val val_acc: 0.7134\n",
      "Val val_f1: 0.6811\n",
      "Val epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.6531: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train train_loss: 0.1262\n",
      "Train train_acc: 0.9642\n",
      "Train train_f1: 0.9607\n",
      "Val val_loss: 1.1666\n",
      "Val val_acc: 0.6943\n",
      "Val val_f1: 0.6744\n",
      "Val epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.3960: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train train_loss: 0.1763\n",
      "Train train_acc: 0.9475\n",
      "Train train_f1: 0.9424\n",
      "Val val_loss: 1.1887\n",
      "Val val_acc: 0.7134\n",
      "Val val_f1: 0.6980\n",
      "Val epoch: 5.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 1.5390: 100%|██████████| 40/40 [00:09<00:00,  4.23it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train train_loss: 0.1882\n",
      "Train train_acc: 0.9482\n",
      "Train train_f1: 0.9440\n",
      "Val val_loss: 0.9942\n",
      "Val val_acc: 0.7389\n",
      "Val val_f1: 0.7133\n",
      "Val epoch: 6.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.4077: 100%|██████████| 40/40 [00:09<00:00,  4.18it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train train_loss: 0.1991\n",
      "Train train_acc: 0.9427\n",
      "Train train_f1: 0.9430\n",
      "Val val_loss: 1.0576\n",
      "Val val_acc: 0.7548\n",
      "Val val_f1: 0.7236\n",
      "Val epoch: 7.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.4029: 100%|██████████| 40/40 [00:09<00:00,  4.23it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train train_loss: 0.1361\n",
      "Train train_acc: 0.9642\n",
      "Train train_f1: 0.9620\n",
      "Val val_loss: 0.9420\n",
      "Val val_acc: 0.7707\n",
      "Val val_f1: 0.7392\n",
      "Val epoch: 8.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.2529: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train train_loss: 0.0738\n",
      "Train train_acc: 0.9785\n",
      "Train train_f1: 0.9758\n",
      "Val val_loss: 1.2436\n",
      "Val val_acc: 0.7452\n",
      "Val val_f1: 0.7149\n",
      "Val epoch: 9.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0039: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train train_loss: 0.1240\n",
      "Train train_acc: 0.9634\n",
      "Train train_f1: 0.9590\n",
      "Val val_loss: 0.9847\n",
      "Val val_acc: 0.7516\n",
      "Val val_f1: 0.7209\n",
      "Val epoch: 10.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1140: 100%|██████████| 40/40 [00:09<00:00,  4.09it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train train_loss: 0.0358\n",
      "Train train_acc: 0.9912\n",
      "Train train_f1: 0.9898\n",
      "Val val_loss: 1.0972\n",
      "Val val_acc: 0.7771\n",
      "Val val_f1: 0.7533\n",
      "Val epoch: 11.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.5561: 100%|██████████| 40/40 [00:09<00:00,  4.16it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train train_loss: 0.0658\n",
      "Train train_acc: 0.9841\n",
      "Train train_f1: 0.9847\n",
      "Val val_loss: 1.3139\n",
      "Val val_acc: 0.7389\n",
      "Val val_f1: 0.6962\n",
      "Val epoch: 12.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 2.4915: 100%|██████████| 40/40 [00:09<00:00,  4.28it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train train_loss: 0.2687\n",
      "Train train_acc: 0.9307\n",
      "Train train_f1: 0.9302\n",
      "Val val_loss: 1.4408\n",
      "Val val_acc: 0.7357\n",
      "Val val_f1: 0.7037\n",
      "Val epoch: 13.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 1.1377: 100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train train_loss: 0.2383\n",
      "Train train_acc: 0.9291\n",
      "Train train_f1: 0.9272\n",
      "Val val_loss: 1.3169\n",
      "Val val_acc: 0.7293\n",
      "Val val_f1: 0.7036\n",
      "Val epoch: 14.0000\n",
      "\n",
      "Early stopping\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8458: 100%|██████████| 40/40 [00:09<00:00,  4.30it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train train_loss: 1.8129\n",
      "Train train_acc: 0.4689\n",
      "Train train_f1: 0.4298\n",
      "Val val_loss: 1.1451\n",
      "Val val_acc: 0.6433\n",
      "Val val_f1: 0.6107\n",
      "Val epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 1.3122: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train train_loss: 0.6576\n",
      "Train train_acc: 0.7954\n",
      "Train train_f1: 0.7769\n",
      "Val val_loss: 1.0251\n",
      "Val val_acc: 0.7102\n",
      "Val val_f1: 0.6761\n",
      "Val epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.4842: 100%|██████████| 40/40 [00:09<00:00,  4.20it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train train_loss: 0.3205\n",
      "Train train_acc: 0.8957\n",
      "Train train_f1: 0.8891\n",
      "Val val_loss: 1.1787\n",
      "Val val_acc: 0.7229\n",
      "Val val_f1: 0.7070\n",
      "Val epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1785: 100%|██████████| 40/40 [00:09<00:00,  4.12it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train train_loss: 0.2340\n",
      "Train train_acc: 0.9299\n",
      "Train train_f1: 0.9264\n",
      "Val val_loss: 0.9237\n",
      "Val val_acc: 0.7452\n",
      "Val val_f1: 0.7294\n",
      "Val epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0390: 100%|██████████| 40/40 [00:09<00:00,  4.18it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train train_loss: 0.1539\n",
      "Train train_acc: 0.9459\n",
      "Train train_f1: 0.9412\n",
      "Val val_loss: 1.1961\n",
      "Val val_acc: 0.7229\n",
      "Val val_f1: 0.7022\n",
      "Val epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1017: 100%|██████████| 40/40 [00:09<00:00,  4.13it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train train_loss: 0.0999\n",
      "Train train_acc: 0.9705\n",
      "Train train_f1: 0.9709\n",
      "Val val_loss: 0.8157\n",
      "Val val_acc: 0.7548\n",
      "Val val_f1: 0.7382\n",
      "Val epoch: 5.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0816: 100%|██████████| 40/40 [00:09<00:00,  4.13it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train train_loss: 0.0514\n",
      "Train train_acc: 0.9817\n",
      "Train train_f1: 0.9810\n",
      "Val val_loss: 0.9035\n",
      "Val val_acc: 0.7389\n",
      "Val val_f1: 0.7279\n",
      "Val epoch: 6.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.2238: 100%|██████████| 40/40 [00:09<00:00,  4.16it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train train_loss: 0.0570\n",
      "Train train_acc: 0.9857\n",
      "Train train_f1: 0.9843\n",
      "Val val_loss: 1.1387\n",
      "Val val_acc: 0.7484\n",
      "Val val_f1: 0.7401\n",
      "Val epoch: 7.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1894: 100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train train_loss: 0.0412\n",
      "Train train_acc: 0.9889\n",
      "Train train_f1: 0.9881\n",
      "Val val_loss: 1.0073\n",
      "Val val_acc: 0.7420\n",
      "Val val_f1: 0.7249\n",
      "Val epoch: 8.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.5359: 100%|██████████| 40/40 [00:09<00:00,  4.26it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train train_loss: 0.1430\n",
      "Train train_acc: 0.9578\n",
      "Train train_f1: 0.9559\n",
      "Val val_loss: 1.1518\n",
      "Val val_acc: 0.7611\n",
      "Val val_f1: 0.7296\n",
      "Val epoch: 9.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1554: 100%|██████████| 40/40 [00:09<00:00,  4.14it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train train_loss: 0.2155\n",
      "Train train_acc: 0.9236\n",
      "Train train_f1: 0.9206\n",
      "Val val_loss: 1.0313\n",
      "Val val_acc: 0.7580\n",
      "Val val_f1: 0.7178\n",
      "Val epoch: 10.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.5302: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train train_loss: 0.1534\n",
      "Train train_acc: 0.9562\n",
      "Train train_f1: 0.9553\n",
      "Val val_loss: 0.8711\n",
      "Val val_acc: 0.7803\n",
      "Val val_f1: 0.7584\n",
      "Val epoch: 11.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0540: 100%|██████████| 40/40 [00:09<00:00,  4.28it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train train_loss: 0.1667\n",
      "Train train_acc: 0.9522\n",
      "Train train_f1: 0.9516\n",
      "Val val_loss: 1.1633\n",
      "Val val_acc: 0.7261\n",
      "Val val_f1: 0.7069\n",
      "Val epoch: 12.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0399: 100%|██████████| 40/40 [00:09<00:00,  4.21it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train train_loss: 0.0709\n",
      "Train train_acc: 0.9769\n",
      "Train train_f1: 0.9761\n",
      "Val val_loss: 1.0609\n",
      "Val val_acc: 0.7643\n",
      "Val val_f1: 0.7437\n",
      "Val epoch: 13.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.5763: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train train_loss: 0.0734\n",
      "Train train_acc: 0.9793\n",
      "Train train_f1: 0.9769\n",
      "Val val_loss: 1.1928\n",
      "Val val_acc: 0.7675\n",
      "Val val_f1: 0.7507\n",
      "Val epoch: 14.0000\n",
      "\n",
      "Early stopping\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7601: 100%|██████████| 40/40 [00:09<00:00,  4.11it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train train_loss: 1.7657\n",
      "Train train_acc: 0.4984\n",
      "Train train_f1: 0.4643\n",
      "Val val_loss: 1.5001\n",
      "Val val_acc: 0.5764\n",
      "Val val_f1: 0.5188\n",
      "Val epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.5076: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train train_loss: 0.6699\n",
      "Train train_acc: 0.7818\n",
      "Train train_f1: 0.7634\n",
      "Val val_loss: 1.1824\n",
      "Val val_acc: 0.6369\n",
      "Val val_f1: 0.6115\n",
      "Val epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.7232: 100%|██████████| 40/40 [00:09<00:00,  4.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train train_loss: 0.3080\n",
      "Train train_acc: 0.8997\n",
      "Train train_f1: 0.8924\n",
      "Val val_loss: 1.0175\n",
      "Val val_acc: 0.7452\n",
      "Val val_f1: 0.7207\n",
      "Val epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1356: 100%|██████████| 40/40 [00:09<00:00,  4.28it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train train_loss: 0.1698\n",
      "Train train_acc: 0.9427\n",
      "Train train_f1: 0.9400\n",
      "Val val_loss: 1.0376\n",
      "Val val_acc: 0.7229\n",
      "Val val_f1: 0.7086\n",
      "Val epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1780: 100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train train_loss: 0.1551\n",
      "Train train_acc: 0.9506\n",
      "Train train_f1: 0.9460\n",
      "Val val_loss: 1.3250\n",
      "Val val_acc: 0.6656\n",
      "Val val_f1: 0.6349\n",
      "Val epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.2631: 100%|██████████| 40/40 [00:09<00:00,  4.11it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train train_loss: 0.0865\n",
      "Train train_acc: 0.9737\n",
      "Train train_f1: 0.9728\n",
      "Val val_loss: 1.3825\n",
      "Val val_acc: 0.7261\n",
      "Val val_f1: 0.6824\n",
      "Val epoch: 5.0000\n",
      "\n",
      "Early stopping\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9406: 100%|██████████| 40/40 [00:09<00:00,  4.09it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train train_loss: 1.7924\n",
      "Train train_acc: 0.5008\n",
      "Train train_f1: 0.4691\n",
      "Val val_loss: 1.2955\n",
      "Val val_acc: 0.6274\n",
      "Val val_f1: 0.5704\n",
      "Val epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 2.2246: 100%|██████████| 40/40 [00:09<00:00,  4.20it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train train_loss: 0.6430\n",
      "Train train_acc: 0.8129\n",
      "Train train_f1: 0.7946\n",
      "Val val_loss: 0.9790\n",
      "Val val_acc: 0.7038\n",
      "Val val_f1: 0.6539\n",
      "Val epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.4891: 100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train train_loss: 0.2836\n",
      "Train train_acc: 0.9053\n",
      "Train train_f1: 0.8941\n",
      "Val val_loss: 1.0212\n",
      "Val val_acc: 0.7229\n",
      "Val val_f1: 0.6882\n",
      "Val epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0893: 100%|██████████| 40/40 [00:09<00:00,  4.13it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train train_loss: 0.1865\n",
      "Train train_acc: 0.9451\n",
      "Train train_f1: 0.9411\n",
      "Val val_loss: 0.9818\n",
      "Val val_acc: 0.7293\n",
      "Val val_f1: 0.6986\n",
      "Val epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0132: 100%|██████████| 40/40 [00:09<00:00,  4.12it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train train_loss: 0.1293\n",
      "Train train_acc: 0.9594\n",
      "Train train_f1: 0.9577\n",
      "Val val_loss: 0.9171\n",
      "Val val_acc: 0.7516\n",
      "Val val_f1: 0.7333\n",
      "Val epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 1.0568: 100%|██████████| 40/40 [00:09<00:00,  4.24it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train train_loss: 0.0742\n",
      "Train train_acc: 0.9857\n",
      "Train train_f1: 0.9847\n",
      "Val val_loss: 1.1075\n",
      "Val val_acc: 0.7229\n",
      "Val val_f1: 0.6860\n",
      "Val epoch: 5.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1561: 100%|██████████| 40/40 [00:09<00:00,  4.20it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train train_loss: 0.1502\n",
      "Train train_acc: 0.9522\n",
      "Train train_f1: 0.9529\n",
      "Val val_loss: 1.1567\n",
      "Val val_acc: 0.7261\n",
      "Val val_f1: 0.6905\n",
      "Val epoch: 6.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.0597: 100%|██████████| 40/40 [00:09<00:00,  4.24it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train train_loss: 0.1008\n",
      "Train train_acc: 0.9658\n",
      "Train train_f1: 0.9644\n",
      "Val val_loss: 0.8618\n",
      "Val val_acc: 0.7643\n",
      "Val val_f1: 0.7518\n",
      "Val epoch: 7.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.6073: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train train_loss: 0.0896\n",
      "Train train_acc: 0.9753\n",
      "Train train_f1: 0.9747\n",
      "Val val_loss: 1.2502\n",
      "Val val_acc: 0.7261\n",
      "Val val_f1: 0.6878\n",
      "Val epoch: 8.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.1746: 100%|██████████| 40/40 [00:09<00:00,  4.33it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train train_loss: 0.1956\n",
      "Train train_acc: 0.9387\n",
      "Train train_f1: 0.9351\n",
      "Val val_loss: 1.1113\n",
      "Val val_acc: 0.7166\n",
      "Val val_f1: 0.6942\n",
      "Val epoch: 9.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "Loss: 0.2064: 100%|██████████| 40/40 [00:09<00:00,  4.31it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: divide by zero encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/1655784584.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/tmp/ipykernel_13147/491761787.py:49: RuntimeWarning: invalid value encountered in cast\n",
      "  cam = np.uint8(255 * cam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train train_loss: 0.1113\n",
      "Train train_acc: 0.9578\n",
      "Train train_f1: 0.9552\n",
      "Val val_loss: 1.3829\n",
      "Val val_acc: 0.6911\n",
      "Val val_f1: 0.6444\n",
      "Val epoch: 10.0000\n",
      "\n",
      "Early stopping\n",
      "Best Model from Fold 3 with F1: 0.7583610622257166 and Loss: 0.8710544347763062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:04<00:00, 20.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 함수\n",
    "def evaluate(loader, model, loss_fn, device, target_layer, train_df):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    incorrect_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            for target, pred, img_name in zip(targets_np, preds_np, loader.dataset.df[:, 0]):\n",
    "                if target != pred:\n",
    "                    img_path = os.path.join(loader.dataset.path, img_name)\n",
    "                    actual_target = int(train_df[train_df['ID'] == img_name]['target'].values[0])\n",
    "                    incorrect_samples.append((actual_target, int(pred), img_path))\n",
    "\n",
    "    # 잘못된 예측에 대해 Grad-CAM 시각화\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    os.makedirs(\"/root/incorrect_images_CAM_kkh3/\", exist_ok=True)\n",
    "    for target, pred, img_path in incorrect_samples:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        src_path = os.path.join(\"/root/data/train/\", img_name)  # train 디렉토리에서 이미지 경로\n",
    "        img = Image.open(src_path).convert('RGB')\n",
    "        img = img.resize((img_size, img_size))\n",
    "        img_tensor = transform(image=np.array(img))['image'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Grad-CAM 시각화\n",
    "        cam = grad_cam(img_tensor, class_idx=pred)\n",
    "\n",
    "        # 원본 이미지와 Grad-CAM 결과 시각화\n",
    "        image_np = img_tensor.cpu().data.numpy()[0].transpose(1, 2, 0)\n",
    "        image_np = np.array([0.229, 0.224, 0.225]) * image_np + np.array([0.485, 0.456, 0.406])\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "        # CAM을 원본 이미지 크기에 맞게 변환\n",
    "        cam = np.uint8(255 * cam)\n",
    "        cam = np.uint8(Image.fromarray(cam).resize((image_np.shape[1], image_np.shape[0]), Image.LANCZOS))\n",
    "\n",
    "        # OpenCV로 색상 맵 적용\n",
    "        heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "\n",
    "        # BGR을 RGB로 변환\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 원본 이미지 위에 히트맵 겹치기\n",
    "        superimposed_img = heatmap * 0.4 + np.uint8(image_np * 255)\n",
    "\n",
    "        # 이미지를 0-1 범위로 정규화\n",
    "        superimposed_img = np.clip(superimposed_img / 255.0, 0, 1)\n",
    "\n",
    "        # 시각화 이미지 저장\n",
    "        cam_path = os.path.join(\"/root/incorrect_images_CAM_kkh3/\", f\"cam_{target}_{pred}_{img_name}\")\n",
    "        plt.imsave(cam_path, superimposed_img)\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"incorrect_samples\": incorrect_samples\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# Hyper-parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '/root/data/'\n",
    "model_name = 'efficientnet_b0'\n",
    "img_size = 224\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "n_splits = 5\n",
    "patience = 3\n",
    "\n",
    "# Transform 정의\n",
    "transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"/root/data/train.csv\")\n",
    "\n",
    "best_overall_model = None\n",
    "best_overall_f1 = 0\n",
    "best_overall_loss = float('inf')\n",
    "best_fold_idx = -1\n",
    "\n",
    "all_incorrect_samples = []  # 모든 폴드의 잘못된 예측을 저장할 리스트\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_weights = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, \"/root/data/train_aug/\", transform=transform)\n",
    "    val_dataset = ImageDataset(val_df, \"/root/data/train_aug/\", transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=len(df['target'].unique())\n",
    "    ).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_f1 = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Grad-CAM 시각화를 위한 모델 준비\n",
    "    target_layer = model.conv_head\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device=device)\n",
    "        val_ret = evaluate(val_loader, model, loss_fn, device=device, target_layer=target_layer, train_df=df)\n",
    "        val_ret['epoch'] = epoch\n",
    "\n",
    "        log = f\"Epoch {epoch + 1}/{EPOCHS}\\n\"\n",
    "        for k, v in train_ret.items():\n",
    "            log += f\"Train {k}: {v:.4f}\\n\"\n",
    "        for k, v in val_ret.items():\n",
    "            if k != 'incorrect_samples':  # 잘못된 예측 샘플은 로그에서 제외\n",
    "                log += f\"Val {k}: {v:.4f}\\n\"\n",
    "        print(log)\n",
    "\n",
    "        if val_ret['val_loss'] < best_val_loss or val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_loss = val_ret['val_loss']\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            torch.save(model.state_dict(), f\"best_model_fold_{fold + 1}_{model_name}.pth\")\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # 현재 폴드의 최상의 F1 점수를 기록\n",
    "    fold_weights.append(val_ret['val_f1'])\n",
    "    # 현재 폴드의 잘못된 예측을 저장\n",
    "    all_incorrect_samples.extend(val_ret['incorrect_samples'])\n",
    "\n",
    "    # 모델별 최상의 성능 비교 및 업데이트\n",
    "    if best_val_f1 > best_overall_f1 and best_val_loss < best_overall_loss:\n",
    "        best_overall_f1 = best_val_f1\n",
    "        best_overall_loss = best_val_loss\n",
    "        best_fold_idx = fold + 1\n",
    "\n",
    "# 잘못된 예측 결과를 저장\n",
    "incorrect_df = pd.DataFrame(all_incorrect_samples, columns=['target', 'pred', 'img_path'])\n",
    "incorrect_df.to_csv(\"/root/incorrect_predictions_kkh3.csv\", index=False)\n",
    "\n",
    "print(f\"Best Model from Fold {best_fold_idx} with F1: {best_overall_f1} and Loss: {best_overall_loss}\")\n",
    "\n",
    "# 예측 및 결과 저장\n",
    "test_df = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
    "tst_dataset = ImageDataset(test_df, \"/root/data/test/\", transform=transform)\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=len(df['target'].unique())\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"best_model_fold_{best_fold_idx}_{model_name}.pth\"))\n",
    "model.eval()\n",
    "\n",
    "fold_preds = []\n",
    "for image, _ in tqdm(tst_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    fold_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "fold_preds = np.concatenate(fold_preds, axis=0)\n",
    "final_preds = np.argmax(fold_preds, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame(test_df, columns=['ID'])\n",
    "pred_df['target'] = final_preds\n",
    "sample_submission_df = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()\n",
    "pred_df.to_csv(\"/root/efficient_net_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
